{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR 10 Abalation study\n",
    "\n",
    "> 1) LeNET scratch (**59%**)\n",
    "\n",
    "> 2) Use MaxPool instead of Average pool (Slight improvement : **61%** )\n",
    "\n",
    "> 3) Use ReLu instead of TanH (Fast training but not much improvement)\n",
    "\n",
    "> 4) L2 regularizer with Relu (Slight improvement : **63%**)\n",
    "\n",
    "> 5) Increase convolution filters (Improves to **70%**, which is the benchmark for model of this size and its training settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predict:'eg: [2,4,1,...]',true: 'eg: [2,4,1,...]') -> int:\n",
    "    correct_pred = tf.equal(predict,true)\n",
    "    #We have to cast [True,False,True,...] --> [1,0,1...]\n",
    "    acc = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "    return acc\n",
    "\n",
    "file_path = \"datasets/cifar-10-batches-bin/data_batch_*\"\n",
    "train_bins = glob.glob(file_path)\n",
    "\n",
    "\n",
    "def cifar_dataset(files_list:list) -> tf.data.Dataset:\n",
    "\n",
    "    data = tf.data.FixedLengthRecordDataset(files_list,1+3*32*32)\n",
    "    data = data.map(lambda x: tf.decode_raw(x,tf.uint8),num_parallel_calls=4)\n",
    "    data = data.map(lambda x: (x[1:],tf.expand_dims(x[0],0)),num_parallel_calls=4) \n",
    "    data = data.map(lambda x,y: (tf.reshape(x,(3,32,32)),y),num_parallel_calls=4)\n",
    "    data = data.map(lambda x,y: (tf.transpose(x,(1,2,0)),y),num_parallel_calls=4)\n",
    "    data = data.map(lambda x,y: (tf.image.convert_image_dtype(x,tf.float32),y),num_parallel_calls=4)\n",
    "        \n",
    "    return data\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    train_dataset = cifar_dataset(train_bins)\n",
    "    train_dataset = train_dataset.shuffle(20000)\n",
    "    train_dataset = train_dataset.repeat(20)\n",
    "    train_dataset = train_dataset.batch(10)\n",
    "    train_dataset = train_dataset.prefetch(2)\n",
    "\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle,train_dataset.output_types,train_dataset.output_shapes)\n",
    "model_input,label = iterator.get_next()\n",
    "\n",
    "train_iterator = train_dataset.make_one_shot_iterator()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    test_dataset = cifar_dataset(['datasets/cifar-10-batches-bin/test_batch.bin'])\n",
    "    test_dataset = test_dataset.batch(10)\n",
    "    test_dataset = test_dataset.prefetch(2)\n",
    "    \n",
    "test_iterator = test_dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorShape([Dimension(None), Dimension(32), Dimension(32), Dimension(3)]),\n",
       "  TensorShape([Dimension(None), Dimension(1)])),\n",
       " (tf.float32, tf.uint8))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.output_shapes,train_dataset.output_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = tf.placeholder(tf.bool)\n",
    "def trainval():\n",
    "    import time\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        #Training\n",
    "        hdl = sess.run(train_iterator.string_handle())   \n",
    "        start = time.time()\n",
    "        try:\n",
    "            i = 1\n",
    "            tmp = []\n",
    "            while True:\n",
    "                i = i+1\n",
    "                l,_ = sess.run([loss,train],{handle:hdl,DROPOUT:True})\n",
    "                tmp.append(l)\n",
    "                if i%5000 == 0:\n",
    "                    avg_loss = np.array(tmp).mean()\n",
    "                    print(\"Batch: \",i,avg_loss)\n",
    "                    tmp = []\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "\n",
    "        end = time.time()\n",
    "        elapsed = end-start\n",
    "        print(\"Elapsed time : \", elapsed, \" s\")\n",
    "\n",
    "        #Testing\n",
    "        print(\"Testing : \")\n",
    "        hdl = sess.run(test_iterator.string_handle())    \n",
    "        acc = get_accuracy(tf.argmax(classifier,axis=1),tf.transpose(tf.argmax(tf.one_hot(label,10),axis=2)))\n",
    "\n",
    "        try:\n",
    "            i = 0\n",
    "            acc_list = []\n",
    "            while True:\n",
    "                i = i+1\n",
    "                a = sess.run(acc,{handle:hdl,DROPOUT:False})\n",
    "                acc_list.append(a)\n",
    "                if i%100 == 0:\n",
    "                    print(i, \"Mean Acc : \", np.array(acc_list).mean())\n",
    "                    acc_list = []\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: LeNET Scratch\n",
    "\n",
    "> Base Accuracy : **59%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_features(model_input):\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(model_input,6,(5,5),(1,1),activation=tf.nn.tanh,name=\"Conv1\")\n",
    "    avgpool1 = tf.layers.average_pooling2d(conv1,(2,2),(2,2),name=\"AvgPool1\")\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(avgpool1,16,(5,5),(1,1),activation=tf.nn.tanh,name=\"Conv2\")\n",
    "    avgpool2 = tf.layers.average_pooling2d(conv2,(2,2),(2,2),name=\"AvgPool2\")\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(avgpool2,120,(5,5),(1,1),activation=tf.nn.tanh,name=\"Conv3\")\n",
    "    \n",
    "    flatten = tf.layers.Flatten()(conv3)\n",
    "    \n",
    "    features = tf.layers.dense(flatten,84,activation=tf.nn.tanh,name=\"Dense_84\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = cnn_features(model_input)\n",
    "classifier = tf.layers.dense(features,10,name=\"Dense_10\")\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(label,10),logits=classifier))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  5000 1.8475738\n",
      "Batch:  10000 1.5774492\n",
      "Batch:  15000 1.4366148\n",
      "Batch:  20000 1.3506023\n",
      "Batch:  25000 1.2912809\n",
      "Batch:  30000 1.2428441\n",
      "Batch:  35000 1.2014774\n",
      "Batch:  40000 1.164475\n",
      "Batch:  45000 1.1318848\n",
      "Batch:  50000 1.100357\n",
      "Batch:  55000 1.0726489\n",
      "Batch:  60000 1.0479746\n",
      "Batch:  65000 1.0217873\n",
      "Batch:  70000 0.9982334\n",
      "Batch:  75000 0.97316056\n",
      "Batch:  80000 0.9523098\n",
      "Batch:  85000 0.928825\n",
      "Batch:  90000 0.90928185\n",
      "Batch:  95000 0.8877957\n",
      "Batch:  100000 0.8699004\n",
      "Elapsed time :  132.51710152626038  s\n",
      "Testing : \n",
      "100 Mean Acc :  0.5989999\n",
      "200 Mean Acc :  0.59700006\n",
      "300 Mean Acc :  0.589\n",
      "400 Mean Acc :  0.602\n",
      "500 Mean Acc :  0.60300004\n",
      "600 Mean Acc :  0.574\n",
      "700 Mean Acc :  0.573\n",
      "800 Mean Acc :  0.608\n",
      "900 Mean Acc :  0.5799999\n",
      "1000 Mean Acc :  0.5870001\n"
     ]
    }
   ],
   "source": [
    "trainval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: LeNET but with Maxpool\n",
    "\n",
    "> Slight improvement (**61%**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_features(model_input):\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(model_input,6,(5,5),(1,1),activation=tf.nn.tanh,name=\"Conv1\")\n",
    "    maxpool1 = tf.layers.max_pooling2d(conv1,(2,2),(2,2),name=\"MaxPool1\")\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(maxpool1,16,(5,5),(1,1),activation=tf.nn.tanh,name=\"Conv2\")\n",
    "    maxpool2 = tf.layers.max_pooling2d(conv2,(2,2),(2,2),name=\"MaxPool2\")\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(maxpool2,120,(5,5),(1,1),activation=tf.nn.tanh,name=\"Conv3\")\n",
    "    \n",
    "    flatten = tf.layers.Flatten()(conv3)\n",
    "    \n",
    "    features = tf.layers.dense(flatten,84,activation=tf.nn.tanh,name=\"Dense_84\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "features = cnn_features(model_input)\n",
    "classifier = tf.layers.dense(features,10,name=\"Dense_10\")\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(label,10),logits=classifier))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  5000 1.6675485\n",
      "Batch:  10000 1.3732225\n",
      "Batch:  15000 1.2616618\n",
      "Batch:  20000 1.1835613\n",
      "Batch:  25000 1.1243168\n",
      "Batch:  30000 1.0753567\n",
      "Batch:  35000 1.0307881\n",
      "Batch:  40000 0.9922322\n",
      "Batch:  45000 0.9570381\n",
      "Batch:  50000 0.92365265\n",
      "Batch:  55000 0.8934568\n",
      "Batch:  60000 0.8650314\n",
      "Batch:  65000 0.8358092\n",
      "Batch:  70000 0.81065595\n",
      "Batch:  75000 0.7840318\n",
      "Batch:  80000 0.7584776\n",
      "Batch:  85000 0.7320794\n",
      "Batch:  90000 0.708358\n",
      "Batch:  95000 0.6857122\n",
      "Batch:  100000 0.66245407\n",
      "Elapsed time :  131.8530716896057  s\n",
      "Testing : \n",
      "100 Mean Acc :  0.62100005\n",
      "200 Mean Acc :  0.63100004\n",
      "300 Mean Acc :  0.577\n",
      "400 Mean Acc :  0.60999995\n",
      "500 Mean Acc :  0.62600005\n",
      "600 Mean Acc :  0.616\n",
      "700 Mean Acc :  0.608\n",
      "800 Mean Acc :  0.61800003\n",
      "900 Mean Acc :  0.6\n",
      "1000 Mean Acc :  0.62899995\n"
     ]
    }
   ],
   "source": [
    "trainval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: ReLU\n",
    "\n",
    "> ReLU if fast, but does not show improvement without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_features(model_input):\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(model_input,6,(5,5),(1,1),activation=tf.nn.relu,name=\"Conv1\")\n",
    "    maxpool1 = tf.layers.max_pooling2d(conv1,(2,2),(2,2),name=\"MaxPool1\")\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(maxpool1,16,(5,5),(1,1),activation=tf.nn.relu,name=\"Conv2\")\n",
    "    maxpool2 = tf.layers.max_pooling2d(conv2,(2,2),(2,2),name=\"MaxPool2\")\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(maxpool2,120,(5,5),(1,1),activation=tf.nn.relu,name=\"Conv3\")\n",
    "    \n",
    "    flatten = tf.layers.Flatten()(conv3)\n",
    "    \n",
    "    features = tf.layers.dense(flatten,84,activation=tf.nn.relu,name=\"Dense_84\")\n",
    "    \n",
    "    return features\n",
    "\n",
    "features = cnn_features(model_input)\n",
    "classifier = tf.layers.dense(features,10,name=\"Dense_10\")\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(label,10),logits=classifier))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  5000 1.7889365\n",
      "Batch:  10000 1.4177151\n",
      "Batch:  15000 1.2917476\n",
      "Batch:  20000 1.2078041\n",
      "Batch:  25000 1.1424283\n",
      "Batch:  30000 1.0895842\n",
      "Batch:  35000 1.0419424\n",
      "Batch:  40000 0.9986101\n",
      "Batch:  45000 0.9605549\n",
      "Batch:  50000 0.92250764\n",
      "Batch:  55000 0.8892804\n",
      "Batch:  60000 0.85587186\n",
      "Batch:  65000 0.8264523\n",
      "Batch:  70000 0.80096054\n",
      "Batch:  75000 0.77150655\n",
      "Batch:  80000 0.7439104\n",
      "Batch:  85000 0.71845305\n",
      "Batch:  90000 0.69797635\n",
      "Batch:  95000 0.6725598\n",
      "Batch:  100000 0.6565832\n",
      "Elapsed time :  138.70953798294067  s\n",
      "Testing : \n",
      "100 Mean Acc :  0.5889999\n",
      "200 Mean Acc :  0.62\n",
      "300 Mean Acc :  0.588\n",
      "400 Mean Acc :  0.604\n",
      "500 Mean Acc :  0.606\n",
      "600 Mean Acc :  0.613\n",
      "700 Mean Acc :  0.581\n",
      "800 Mean Acc :  0.623\n",
      "900 Mean Acc :  0.58\n",
      "1000 Mean Acc :  0.597\n"
     ]
    }
   ],
   "source": [
    "trainval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: L2 regularization\n",
    "\n",
    "> Slight improvement (**63%**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_features(model_input):\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(model_input,6,(5,5),(1,1),\n",
    "                             kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=0.001), \n",
    "                             activation=tf.nn.relu,\n",
    "                             name=\"Conv1\")\n",
    "    maxpool1 = tf.layers.max_pooling2d(conv1,(2,2),(2,2),name=\"MaxPool1\")\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(maxpool1,16,(5,5),(1,1),\n",
    "                             kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "                             activation=tf.nn.relu,\n",
    "                             name=\"Conv2\")\n",
    "    maxpool2 = tf.layers.max_pooling2d(conv2,(2,2),(2,2),name=\"MaxPool2\")\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(maxpool2,120,(5,5),(1,1),\n",
    "                             kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=0.01), \n",
    "                             activation=tf.nn.relu,\n",
    "                             name=\"Conv3\")\n",
    "    \n",
    "    flatten = tf.layers.Flatten()(conv3)\n",
    "    \n",
    "    features = tf.layers.dense(flatten,84,\n",
    "                               kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=0.01), \n",
    "                               activation=tf.nn.relu,\n",
    "                               name=\"Dense_84\")\n",
    "    \n",
    "    \n",
    "    return features\n",
    "\n",
    "features = cnn_features(model_input)\n",
    "classifier = tf.layers.dense(features,10,name=\"Dense_10\")\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(label,10),logits=classifier))\n",
    "loss += tf.losses.get_regularization_loss()\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  5000 2.2963307\n",
      "Batch:  10000 1.7417562\n",
      "Batch:  15000 1.5815724\n",
      "Batch:  20000 1.5106776\n",
      "Batch:  25000 1.4716152\n",
      "Batch:  30000 1.4410975\n",
      "Batch:  35000 1.4193088\n",
      "Batch:  40000 1.4014232\n",
      "Batch:  45000 1.3863301\n",
      "Batch:  50000 1.3702798\n",
      "Batch:  55000 1.3545686\n",
      "Batch:  60000 1.3375293\n",
      "Batch:  65000 1.3281711\n",
      "Batch:  70000 1.3186255\n",
      "Batch:  75000 1.3115855\n",
      "Batch:  80000 1.2989252\n",
      "Batch:  85000 1.2943399\n",
      "Batch:  90000 1.2881706\n",
      "Batch:  95000 1.2838296\n",
      "Batch:  100000 1.278085\n",
      "Elapsed time :  137.675359249115  s\n",
      "Testing : \n",
      "100 Mean Acc :  0.62999994\n",
      "200 Mean Acc :  0.62100005\n",
      "300 Mean Acc :  0.59300005\n",
      "400 Mean Acc :  0.635\n",
      "500 Mean Acc :  0.6619999\n",
      "600 Mean Acc :  0.64199996\n",
      "700 Mean Acc :  0.59699994\n",
      "800 Mean Acc :  0.6319999\n",
      "900 Mean Acc :  0.612\n",
      "1000 Mean Acc :  0.622\n"
     ]
    }
   ],
   "source": [
    "trainval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Increase Filters \n",
    "\n",
    "> **Achieved 70% accuracy**\n",
    "\n",
    "> GPU Util : 60%, GPU Temp: 70 degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_features(model_input):\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(model_input,32,(5,5),(1,1),\n",
    "                             kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=0.001), \n",
    "                             activation=tf.nn.relu,\n",
    "                             name=\"Conv1\")\n",
    "    maxpool1 = tf.layers.max_pooling2d(conv1,(2,2),(2,2),name=\"MaxPool1\")\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(maxpool1,64,(5,5),(1,1),\n",
    "                             kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=0.01),\n",
    "                             activation=tf.nn.relu,\n",
    "                             name=\"Conv2\")\n",
    "    maxpool2 = tf.layers.max_pooling2d(conv2,(2,2),(2,2),name=\"MaxPool2\")\n",
    "    \n",
    "    conv3 = tf.layers.conv2d(maxpool2,120,(5,5),(1,1),\n",
    "                             kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=0.01), \n",
    "                             activation=tf.nn.relu,\n",
    "                             name=\"Conv3\")\n",
    "    \n",
    "    flatten = tf.layers.Flatten()(conv3)\n",
    "    \n",
    "    \n",
    "    features = tf.layers.dense(flatten,592,\n",
    "                               kernel_regularizer= tf.contrib.layers.l2_regularizer(scale=0.01), \n",
    "                               activation=tf.nn.relu,\n",
    "                               name=\"Dense_84\")\n",
    "    \n",
    "    \n",
    "    return features\n",
    "\n",
    "features = cnn_features(model_input)\n",
    "classifier = tf.layers.dense(features,10,name=\"Dense_10\")\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.one_hot(label,10),logits=classifier))\n",
    "loss += tf.losses.get_regularization_loss()\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  5000 2.8987796\n",
      "Batch:  10000 1.9110137\n",
      "Batch:  15000 1.5600703\n",
      "Batch:  20000 1.407018\n",
      "Batch:  25000 1.3275896\n",
      "Batch:  30000 1.2812109\n",
      "Batch:  35000 1.2481775\n",
      "Batch:  40000 1.2185266\n",
      "Batch:  45000 1.197651\n",
      "Batch:  50000 1.1789722\n",
      "Batch:  55000 1.1598485\n",
      "Batch:  60000 1.1452051\n",
      "Batch:  65000 1.1309775\n",
      "Batch:  70000 1.1194522\n",
      "Batch:  75000 1.1119484\n",
      "Batch:  80000 1.106407\n",
      "Batch:  85000 1.0965117\n",
      "Batch:  90000 1.0920885\n",
      "Batch:  95000 1.0879949\n",
      "Batch:  100000 1.0839711\n",
      "Elapsed time :  208.71310687065125  s\n",
      "Testing : \n",
      "100 Mean Acc :  0.7120001\n",
      "200 Mean Acc :  0.7190001\n",
      "300 Mean Acc :  0.69500005\n",
      "400 Mean Acc :  0.7249999\n",
      "500 Mean Acc :  0.726\n",
      "600 Mean Acc :  0.7150001\n",
      "700 Mean Acc :  0.69499993\n",
      "800 Mean Acc :  0.7299999\n",
      "900 Mean Acc :  0.695\n",
      "1000 Mean Acc :  0.68999994\n"
     ]
    }
   ],
   "source": [
    "trainval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
